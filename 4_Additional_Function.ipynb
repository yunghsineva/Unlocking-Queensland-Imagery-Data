{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgkxF8NDiHUI"
      },
      "source": [
        "# Notebook Details\n",
        "This notebook enhances training labels by applying pseudo-labeling to the False Positive (FP) regions identified from the original Mask2Former model, using the Segment Anything Model (SAM) for refinement.\n",
        "\n",
        "At the end of this notebook, a new folder named pseudo_dataset will be generated, containing four folders: (i) train_image (ii) train_mask (iii) val_image (iv) val_mask\n",
        "\n",
        "* train_image: Same images as Input_data/train_image\n",
        "* val_image: GT + pseudo-labeled masks (FP corrected)\n",
        "* train_mask: Same images as Input_data/val_image\n",
        "* val_mask: GT + pseudo-labeled masks (FP corrected)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**IMPORTANT: This notebook is solely for generating pseudo and ground truth (GT) masks. To retrain the model using the enhanced masks, please use \"Notebook 2: Mask2Former\" and update the training input directory from \"Input_data\" to \"pseudo_dataset\".**\n",
        "\n",
        "**Note:**\n",
        "1. \"<<<\" in the code indicates values that you can modify.\n",
        "\n",
        "2. This notebook currently focuses on False Positives only, but can be modified to target False Negatives.\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oCip246kE0t"
      },
      "source": [
        "# Environment Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSGSBBbs-ele",
        "outputId": "b7c30fe5-7f66-4e23-b0a4-bb81eb662c5f"
      },
      "outputs": [],
      "source": [
        "# Install dependencies if not yet installed\n",
        "# Only run this section if needed.\n",
        "\n",
        "# Set UTF-8 locale to avoid encoding-related errors (e.g., with gsutil)\n",
        "import os\n",
        "os.environ[\"LC_ALL\"] = \"C.UTF-8\"\n",
        "os.environ[\"LANG\"] = \"C.UTF-8\"\n",
        "\n",
        "# Install basic dependencies: OpenCV for image processing, matplotlib for visualization\n",
        "!pip install opencv-python matplotlib\n",
        "\n",
        "# Clone the Segment Anything repository from GitHub (Meta AI)\n",
        "!git clone https://github.com/facebookresearch/segment-anything.git\n",
        "%cd segment-anything\n",
        "!pip install -e .\n",
        "\n",
        "# Download the pre-trained SAM (Segment Anything Model) ViT-B weights\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth -O /content/segment-anything/sam_vit_b_01ec64.pth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5R2U6qQyZZoe"
      },
      "outputs": [],
      "source": [
        "#<<< Access your bucket or path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUxe4p23hCJ0"
      },
      "source": [
        "# Utility Functions & Model Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHwY1pgSYeyK"
      },
      "source": [
        "This section sets up important parameters such as image size, confidence thresholds, and Input/Output paths. It also defines two key functions:\n",
        "\n",
        "\n",
        "\n",
        "*   generate_pseudo_negative_mask() for SAM segmentation.\n",
        "*   generate_final_mask() to merge GT and pseudo labels.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yMQIUWphYKCe",
        "outputId": "506ecc11-d413-45e3-9c64-d3a7817b4971"
      },
      "outputs": [],
      "source": [
        "# install all neccessary libraries, delete when neccessary\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEyUdoMqp0U2",
        "outputId": "9e89dbfc-073d-4c6c-c028-04eb61c552ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/110.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# === [Package Locking: ensure stable versions]\n",
        "!pip install -U \"dill==0.3.6\" \"evaluate==0.4.0\" --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Go5_JGvhxwA",
        "outputId": "bc92df7d-3041-4c6d-dda6-bf3a6510ab6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/segment-anything/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(f)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "from segment_anything import SamPredictor, sam_model_registry\n",
        "from transformers import Mask2FormerForUniversalSegmentation, Mask2FormerImageProcessor\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# === Select the appropriate device: GPU if available, otherwise CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# === Load the SAM model using 'vit_b' (base variant) and the pre-trained checkpoint\n",
        "sam = sam_model_registry[\"vit_b\"](checkpoint=\"/content/segment-anything/sam_vit_b_01ec64.pth\").to(device)\n",
        "# === Create a predictor instance to run SAM inference later\n",
        "predictor = SamPredictor(sam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_WZikGlhADO"
      },
      "outputs": [],
      "source": [
        "def generate_pseudo_negative_mask(image, fp_mask, predictor, score_threshold=0.8, area_threshold=100):\n",
        "    \"\"\"\n",
        "    Generate a pseudo negative mask using SAM clipped to FP area.\n",
        "\n",
        "    Args:\n",
        "        image (np.ndarray): RGB image.\n",
        "        fp_mask (np.ndarray): Binary mask where FP area is 255.\n",
        "        predictor (SamPredictor): SAM predictor instance.\n",
        "        score_threshold (float): SAM mask confidence score threshold.\n",
        "        area_threshold (int): Minimum size of connected components to keep.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Pseudo-negative mask (uint8, 0 or 255).\n",
        "    \"\"\"\n",
        "    # Erode FP mask to reduce border noise\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    fp_mask_eroded = cv2.erode(fp_mask, kernel, iterations=1)\n",
        "\n",
        "    predictor.set_image(image)\n",
        "    ys, xs = np.where(fp_mask_eroded == 255)\n",
        "\n",
        "    if len(xs) == 0:\n",
        "        return np.zeros_like(fp_mask)\n",
        "\n",
        "    # Bounding box as prompt\n",
        "    x1, y1, x2, y2 = np.min(xs), np.min(ys), np.max(xs), np.max(ys)\n",
        "    input_box = np.array([x1, y1, x2, y2])\n",
        "\n",
        "    masks, scores, _ = predictor.predict(\n",
        "        box=input_box[None, :],\n",
        "        multimask_output=True\n",
        "    )\n",
        "\n",
        "    best_mask = masks[np.argmax(scores)]\n",
        "    best_score = np.max(scores)\n",
        "\n",
        "    if best_score < score_threshold:\n",
        "        return np.zeros_like(fp_mask)\n",
        "\n",
        "    # Only keep confident SAM mask clipped by FP\n",
        "    pseudo_negative_mask = np.logical_and(best_mask, fp_mask > 0).astype(np.uint8) * 255\n",
        "\n",
        "    # Remove small noise areas\n",
        "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(pseudo_negative_mask, connectivity=8)\n",
        "    cleaned_mask = np.zeros_like(pseudo_negative_mask)\n",
        "    for i in range(1, num_labels):\n",
        "        area = stats[i, cv2.CC_STAT_AREA]\n",
        "        if area > area_threshold:\n",
        "            cleaned_mask[labels == i] = 255\n",
        "\n",
        "    return cleaned_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dx0TdcuQoh_7"
      },
      "outputs": [],
      "source": [
        "def generate_final_mask(gt_mask, pred_mask, pseudo_negative_mask, save_path=None):\n",
        "    \"\"\"\n",
        "    Combine GT and pseudo-negative mask into final mask and optionally save it.\n",
        "\n",
        "    Args:\n",
        "        gt_mask (np.ndarray): Ground truth binary mask (0/1).\n",
        "        pred_mask (np.ndarray): Predicted mask from model (0/1).\n",
        "        pseudo_negative_mask (np.ndarray): Pseudo mask from SAM (0/255).\n",
        "        save_path (str, optional): Path to save the final mask. If None, mask is not saved.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Final mask for training (values: 0, 1, 255).\n",
        "    \"\"\"\n",
        "    # Shape alignment\n",
        "    if pseudo_negative_mask.shape != gt_mask.shape:\n",
        "        pseudo_negative_mask = cv2.resize(\n",
        "            pseudo_negative_mask,\n",
        "            (gt_mask.shape[1], gt_mask.shape[0]),\n",
        "            interpolation=cv2.INTER_NEAREST\n",
        "        )\n",
        "\n",
        "    final_mask = np.full_like(gt_mask, fill_value=255)\n",
        "\n",
        "    # Keep true buildings\n",
        "    final_mask[gt_mask == 1] = 1\n",
        "\n",
        "    # Replace confident FP regions with background (0)\n",
        "    fp_area = ((pred_mask == 1) & (gt_mask == 0))\n",
        "    sam_fp_area = ((pseudo_negative_mask == 255) & fp_area)\n",
        "    final_mask[sam_fp_area] = 0\n",
        "\n",
        "    # Optional save\n",
        "    if save_path is not None:\n",
        "      os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "      cv2.imwrite(save_path, final_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTwBv9NeK2wt"
      },
      "source": [
        "# Pseudo-Label Generation Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3oDNI6vaX_b"
      },
      "source": [
        "This section performs the core logic:\n",
        "\n",
        "Loads images and GT masks from Input_data\n",
        "\n",
        "*   Runs Mask2Former to generate predictions\n",
        "*   Computes mIoU and identifies poor predictions\n",
        "*   Applies SAM on FP regions when necessary\n",
        "*   Saves results into pseudo_dataset\n",
        "\n",
        "Both train and val sets are processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-bmLNQSZgkN",
        "outputId": "f5859d27-b169-43b6-c803-823cb3bfde48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Return to the root content directory (to avoid path issues)\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250,
          "referenced_widgets": [
            "0cbc3ebb413748cf955d4ce50ae83601",
            "287cf27101334ea284f282c11604d4ec",
            "9972acca68cf4727aaf9ff8d16399aee",
            "1baa5d3f992d4e4c96d3c9bdc81f1a15",
            "0f7ff09356054903bf8e9b9709b47a01",
            "3054e6daa0894c46b3e0eb75f711e338",
            "71e24512781d43f7955fb3f1d40b32a1",
            "447848f75d2e4b5abe2f632f5c883d2a",
            "2ed7461d101c4b06877a90c1a7c53dd5",
            "21f3af9833d744d7b29f6b0eb53b7b31",
            "198e204602b04d01914d89db68dcf150"
          ]
        },
        "id": "cBATE9ln6wje",
        "outputId": "d54a9b58-a0fe-4121-c9ea-46a46a708158"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0cbc3ebb413748cf955d4ce50ae83601",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing train set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▎         | 22/600 [00:11<04:08,  2.33it/s]/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:260: RuntimeWarning: invalid value encountered in divide\n",
            "  acc = total_area_intersect / total_area_label\n",
            "  9%|▊         | 52/600 [00:23<03:01,  3.01it/s]/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:259: RuntimeWarning: invalid value encountered in divide\n",
            "  iou = total_area_intersect / total_area_union\n",
            "100%|██████████| 600/600 [03:35<00:00,  2.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing val set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [01:11<00:00,  2.80it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Default parameters\n",
        "miou_threshold = 0.7           #<<< Skip SAM if mIoU is already good\n",
        "score_threshold = 0.8          #<<< Confidence threshold for SAM masks\n",
        "area_threshold = 100           #<<< Minimum area (in pixels) for valid SAM regions\n",
        "image_size = (256, 256)        #<<< Resize target size for all inputs\n",
        "\n",
        "# Load trained Mask2Former model (modify path if needed)\n",
        "inf_model = Mask2FormerForUniversalSegmentation.from_pretrained(\"outputs/model_iou\").to(device).eval()\n",
        "\n",
        "# Image processor to format input image\n",
        "processor = Mask2FormerImageProcessor()\n",
        "\n",
        "# Evaluation metric (e.g., mIoU)\n",
        "infer_metric = evaluate.load(\"mean_iou\")\n",
        "\n",
        "# === Process both train and val sets\n",
        "for split in [\"train\", \"val\"]:\n",
        "    print(f\"\\nProcessing {split} set...\")\n",
        "\n",
        "    input_img_dir = os.path.join(\"Input_data\", f\"{split}_image\")\n",
        "    input_mask_dir = os.path.join(\"Input_data\", f\"{split}_mask\")\n",
        "\n",
        "    output_img_dir = os.path.join(\"pseudo_dataset\", f\"{split}_image\")\n",
        "    output_mask_dir = os.path.join(\"pseudo_dataset\", f\"{split}_mask\")\n",
        "\n",
        "    os.makedirs(output_img_dir, exist_ok=True)\n",
        "    os.makedirs(output_mask_dir, exist_ok=True)\n",
        "\n",
        "    image_files = sorted(os.listdir(input_img_dir))\n",
        "\n",
        "    for img_file in tqdm(image_files):\n",
        "        # === 1. Load image and GT mask\n",
        "        img_path = os.path.join(input_img_dir, img_file)\n",
        "        mask_path = os.path.join(input_mask_dir, img_file.replace(\"img\", \"mask\"))\n",
        "\n",
        "        orig_image = cv2.imread(img_path)\n",
        "        orig_image = cv2.resize(orig_image, image_size)\n",
        "        image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        gt_mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        gt_mask = cv2.resize(gt_mask, image_size)\n",
        "\n",
        "        # === 2. Predict with Mask2Former\n",
        "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "        outputs = inf_model(**inputs)\n",
        "        labels = processor.post_process_semantic_segmentation(outputs, target_sizes=[image_size])[0]\n",
        "        pred_mask = labels.cpu().numpy()\n",
        "\n",
        "        # === 3. Compute mIoU\n",
        "        infer_metric.add_batch(references=[gt_mask], predictions=[pred_mask])\n",
        "        miou = infer_metric.compute(num_labels=2, ignore_index=255, reduce_labels=False)['mean_iou']\n",
        "\n",
        "        # === 4. Save original mask if mIoU >= threshold\n",
        "        out_img_path = os.path.join(output_img_dir, img_file)\n",
        "        out_mask_path = os.path.join(output_mask_dir, img_file.replace(\"img\", \"mask\"))\n",
        "\n",
        "        if miou >= miou_threshold:\n",
        "            cv2.imwrite(out_img_path, orig_image)\n",
        "            cv2.imwrite(out_mask_path, gt_mask)\n",
        "            continue\n",
        "\n",
        "        # === 5. Generate pseudo-negative mask using SAM\n",
        "        # Default: False Positive (predicted as 1, but GT is 0)\n",
        "        fp_mask = ((pred_mask == 1) & (gt_mask == 0)).astype(np.uint8) * 255\n",
        "        # Optional: Target False Negatives instead (predicted as 0, but GT is 1)\n",
        "        # fn_mask = ((pred_mask == 0) & (gt_mask == 1)).astype(np.uint8) * 255\n",
        "\n",
        "        pseudo_negative_mask = generate_pseudo_negative_mask(\n",
        "            image=image,\n",
        "            fp_mask=fp_mask,\n",
        "            predictor=predictor,\n",
        "            score_threshold=score_threshold,\n",
        "            area_threshold=area_threshold\n",
        "        )\n",
        "\n",
        "        # === 6. Merge GT and pseudo mask, then save\n",
        "        cv2.imwrite(out_img_path, orig_image)\n",
        "\n",
        "        final_mask = generate_final_mask(\n",
        "            gt_mask=gt_mask,\n",
        "            pred_mask=pred_mask,\n",
        "            pseudo_negative_mask=pseudo_negative_mask,\n",
        "            save_path=out_mask_path\n",
        "        )\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "XNZDyXvXqkzk"
      ],
      "gpuType": "L4",
      "name": "4. Additional Function.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0cbc3ebb413748cf955d4ce50ae83601": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_287cf27101334ea284f282c11604d4ec",
              "IPY_MODEL_9972acca68cf4727aaf9ff8d16399aee",
              "IPY_MODEL_1baa5d3f992d4e4c96d3c9bdc81f1a15"
            ],
            "layout": "IPY_MODEL_0f7ff09356054903bf8e9b9709b47a01"
          }
        },
        "0f7ff09356054903bf8e9b9709b47a01": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "198e204602b04d01914d89db68dcf150": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1baa5d3f992d4e4c96d3c9bdc81f1a15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21f3af9833d744d7b29f6b0eb53b7b31",
            "placeholder": "​",
            "style": "IPY_MODEL_198e204602b04d01914d89db68dcf150",
            "value": " 13.1k/13.1k [00:00&lt;00:00, 815kB/s]"
          }
        },
        "21f3af9833d744d7b29f6b0eb53b7b31": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "287cf27101334ea284f282c11604d4ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3054e6daa0894c46b3e0eb75f711e338",
            "placeholder": "​",
            "style": "IPY_MODEL_71e24512781d43f7955fb3f1d40b32a1",
            "value": "Downloading builder script: 100%"
          }
        },
        "2ed7461d101c4b06877a90c1a7c53dd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3054e6daa0894c46b3e0eb75f711e338": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "447848f75d2e4b5abe2f632f5c883d2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71e24512781d43f7955fb3f1d40b32a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9972acca68cf4727aaf9ff8d16399aee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_447848f75d2e4b5abe2f632f5c883d2a",
            "max": 13077,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2ed7461d101c4b06877a90c1a7c53dd5",
            "value": 13077
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
